<!DOCTYPE html><html lang="en-US"><head><title>The Computational Linguistics and Cognitive Sciences (CLCS) Lab : Projects</title><meta charset="utf-8"><meta name="author" content="Yang Xu"><meta name="keywords" content="computational linguistics, cognitive sciences, psycholinguistics, NLP, Computer Science, SDSU, San Diego State University, Yang Xu"><link rel="stylesheet" href="css/bootstrap.min.css"><link rel="stylesheet" href="css/style.css"></head><body><div class="container menu"><div class="nav"><a class="active" href="index.html">Home</a><span class="nav-item-sep">|</span><a href="projects.html">Projects</a><span class="nav-item-sep">|</span><a href="people.html">People</a><span class="nav-item-sep">|</span><a href="publications.html">Publications</a></div></div><div class="container"><div class="project col-md-12"><h2>Information-theoretic analysis of non-verbal communication</h2><p>We talk with not just verbal languages, but also non-verbal ones: hand gestures, body poses, head motions, facial expressions. How rich is the information contained in these non-verbal channels? Is there a way to quantify it? Can we understand the meanings of them (gestures, poses, motions etc.) in the same way that we understand natural language, with the help of machine learning tools? </p><p>A simple assumption: if we can extract discrete patterns in non-verbal behaviors, for instance, poses, which is robostly observable across various communication scenarios, doesn't that mean these patterns can be considered as <i>words</i>? We examined the feature space of the body pose of a professor in his Zoom class, with the techniques of <i>body pose estimation</i> and <i>pose embeddings</i>. <i>K</i>-means clustering and <i>t</i>-SNE showed that there are very salient patterns in the professor's body poses. </p><p>This project is funded by NSF (CRII, <strong>award number 2105192</strong>, <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2105192">link</a>)</p><hr></div><div class="project col-md-12"><h2>Multi-modal machine learning models for online educational conversations</h2><p>The world is in great demand of convenient technology to support large scale online learning and conferences, especially under the impact of COVID-19.</p><p>However, there is still large space for improvement in the current remote conference technology. The biggest challenge is information overload â€“ there is too many faces shown in one screen that the instructor cannot easily recognize those who need help or special attention. Secondly, many critical details, such as micro expression, intonation, are not easily captured, which sometimes leaves the actual intention of students not conveyed. </p><img src="img/multi_participants_emotions.jpg" alt="" style="height:80%;width:80%"><p>The state-of-the-art artificial intelligence (A.I.) technology is capable of capturing the subjectivity in human activities. We propose to develop an AI-based system that helps the instructor obtain a better awareness of the overall communication situation, so as to create a more inclusive/engaging learning experience.</p><p>This project is funded by San Diego State University <i>Big Ideas</i> initiative, 2021. The project demonstration is covered here: <a href="https://president.sdsu.edu/moving-forward/big-ideas/proposals/smartinteract">SmartInteract: Artificial Intelligence for Online Teaching Feedback</a></p><hr></div></div></body></html>