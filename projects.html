<!DOCTYPE html><html lang="en-US"><head><title>The Computational Linguistics and Cognitive Sciences (CLCS) Lab : Projects</title><meta charset="utf-8"><meta name="author" content="Yang Xu"><meta name="keywords" content="computational linguistics, cognitive sciences, psycholinguistics, NLP, Computer Science, SDSU, San Diego State University, Yang Xu"><link rel="stylesheet" href="css/bootstrap.min.css"><link rel="stylesheet" href="css/styles.css"></head><body><div class="container menu"><div class="nav"><a class="active" href="index.html">Home</a><span class="nav-item-sep">|</span><a href="projects.html">Projects</a><span class="nav-item-sep">|</span><a href="people.html">People</a><span class="nav-item-sep">|</span><a href="publications.html">Publications</a></div></div><div class="container"><div class="project col-md-12"><h2>Information-theoretic analysis of non-verbal communication</h2><p>We talk with not just verbal languages, but also non-verbal ones: hand gestures, body poses, head motions, facial expressions. How rich is the information contained in these non-verbal channels? Is there a way to quantify it? Can we understand the meanings of them (gestures, poses, motions etc.) in the same way that we understand natural language, with the help of machine learning tools? </p><p>A simple assumption: if we can extract discrete patterns in non-verbal behaviors, for instance, poses, which is robostly observable across various communication scenarios, doesn't that mean these patterns can be considered as <i>words</i>? We examined the feature space of the body pose of a professor in his Zoom class, with the techniques of <i>body pose estimation</i> and <i>pose embeddings</i>. <i>K</i>-means clustering and <i>t</i>-SNE showed that there are very salient patterns in the professor's body poses. </p><p>This project is funded by NSF (CRII, <strong>award number 2105192</strong>, <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2105192">link</a>)</p><p>Checkout the <a href="#">project site</a> for more details. Dataset collected in this project: <a href="#">Life-Lessons</a>.</p><p>Papers published: </p><ul><li><a href="publications.html#2022Gestures1">Gestures Are Used Rationally: Information Theoretic Evidence from Neural Sequential Models</a></li></ul><hr></div><div class="project col-md-12"> <h2>Computer vision and machine learning techniques for smart microvascular rheology studies</h2><p>The  use  of  in-vivo  microvascular  capillaroscopy  has  evolved  into  an  accepted  method  of describing the intravascular milieu from both hemorheological and microangiopathic perspectives. Disorders such as stroke and various forms of coagulopathy such as that related to COVID-19 are strongly correlated with abnormal hemorheological metrics. Historically, in-vivo assessment of erythrocyte passage within the microvascular lumen has been conducted by iterations of a space-time analysis in which erythrocyte location is transformed on a coordinate system then interpreted by  evaluation  of  instantaneous  velocity  of  a  single  erythrocyte  temporally.  The  resultant  output can be represented visually by a series of sloped lines indicating factors such as overall velocity, tube hematocrit and flux. While metrics such as erythrocyte velocity and flux provide important hemorheological  measures  of  cardiovascular  hemodynamics,  the  space  time  analytic  approach negates assessment  of  an  important  corollary  to  erythrocyte  velocity,  namely  erythrocytic aggregate metrics and assessment of flow field homogeneity. </p><p>We  utilized  optical  and  manual techniques as well as a technique which we term transversal temporal cross-correlation (TTC) to observe and measure both erythrocyte velocity and aggregation.</p><p>We demonstrated  that  the  optical  flow  and  TTC  analyses  can  be  used  to estimate   erythrocyte   velocity   and   aggregation   both in ex-vivo microfluidics   laboratory experiments as well as in-vivo recordings.</p><p>Papers published: </p><ul><li><a href="publications.html#2022OpticalFlow">Hierarchical data visualization of experimental erythrocyte aggregation employing cross correlation and optical flow applications</a></li></ul><hr></div><div class="project col-md-12"><h2>Multi-modal machine learning models for online educational conversations</h2><p>The world is in great demand of convenient technology to support large scale online learning and conferences, especially under the impact of COVID-19.</p><p>However, there is still large space for improvement in the current remote conference technology. The biggest challenge is information overload â€“ there is too many faces shown in one screen that the instructor cannot easily recognize those who need help or special attention. Secondly, many critical details, such as micro expression, intonation, are not easily captured, which sometimes leaves the actual intention of students not conveyed. </p><img src="img/multi_participants_emotions.jpg" alt="" style="height:80%;width:80%"><p>The state-of-the-art artificial intelligence (A.I.) technology is capable of capturing the subjectivity in human activities. We propose to develop an AI-based system that helps the instructor obtain a better awareness of the overall communication situation, so as to create a more inclusive/engaging learning experience.</p><p>This project is funded by San Diego State University <i>Big Ideas</i> initiative, 2021. The project demonstration is covered here: <a href="https://president.sdsu.edu/moving-forward/big-ideas/proposals/smartinteract">SmartInteract: Artificial Intelligence for Online Teaching Feedback</a></p><hr></div></div></body></html>